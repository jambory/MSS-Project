{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Efficient Implementation\n",
    "\n",
    "In nn_basic.ipybb, I was able to make a working neural network using just for loops and functions however, there were a couple of large issues with the implementation. For one, it didn't actually have any way to implement more than one hidden layer so it was limited to being a single layer MLP. Also there was not a lot of room for improving the neural network with more features and additions over time. Lastly the algorithm was incredibly ineffecient as all the mathematics were done using for loops, when in reality it is much quicker to use a C wrapper like numpy to calculate these derivatives with matrix operations. \n",
    "\n",
    "This implemetation will be directly following the implementation made in this video:\n",
    "\n",
    "https://www.youtube.com/watch?v=pauPCy_s0Ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Layer Implementation:\n",
    "\n",
    "The base layer is the layer that simply takes in the input of the model and stores them as some activation for individual neurons to be used in the next layer; the Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "    def forward(self, input):\n",
    "        # To do: forward propagate with given input\n",
    "        pass\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO : propagate backwards\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Implementation:\n",
    "\n",
    "The dense layer connects every single neuron in the base layer to a weight to form a weighted some calculation for a neuron's activation in this next dense layer. \n",
    "For the mathematical notation note that:\n",
    "- $i$ denotes amount of neurons in the previous layer\n",
    "- $j$ denotes amount of neurons to be made in this dense layer.\n",
    "- $w_{ij}$ denotes the weight for the ith input into the jth output neuron.\n",
    "- $b_j$ denotes the bias for the jth output neuron.\n",
    "- ${x_i}$ denotes the set of input values from the previous layer\n",
    "- ${y_j}$ denotes the set of output values from this dense layer.\n",
    "\n",
    "You can write a given output as: $y_j = x_1w_{1j} + ... + x_1w_{ij} + b_j$\n",
    "\n",
    "However using matrix multiplication you can rewrite this as:\n",
    "- $W_j$ denotes the $j$ x $i$ matrix that stores all the weight values for a output neuron.\n",
    "- $b$ denotes a vector of bias terms.\n",
    "\n",
    "$$Y = W_jX + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights= np.random.randn(output_size,input_size) # returns array of size (j,i) sampled randomly from standard normal distribution.\n",
    "        self.bias=np.random.randn(output_size, 1) # returns array of size (j,1) sampled randomly from standard normal distribution.\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.inputs) + self.bias # performs caclulation from formula above.\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO : propagate backwards\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation:\n",
    "\n",
    "- $\\frac{\\delta E}{\\delta W} = \\frac{\\delta E}{\\delta Y} \\bullet X^T$\n",
    "\n",
    "- $\\frac{\\delta E}{\\delta B} = \\frac{\\delta E}{\\delta Y}$\n",
    "\n",
    "- $\\frac{\\delta E}{\\delta X} = W^T \\bullet \\frac{\\delta E}{\\delta Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights= np.random.randn(output_size,input_size) # returns array of size (j,i) sampled randomly from standard normal distribution.\n",
    "        self.bias=np.random.randn(output_size, 1) # returns array of size (j,1) sampled randomly from standard normal distribution.\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias # performs caclulation from formula above.\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        self.weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        self.weights -= learning_rate * self.weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        \n",
    "        return np.dot(self.weights.T, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Activation Layer:\n",
    "\n",
    "Pretty easy to notice the lack of an activation function in the implementation of the last type of layer. To address this you create a new type of layer that allows for easy customization makes sense when you think about it. An activation layer is really just a very specific kind of layer where every neuron in the last layer is connected to a single neuron with a given activation function (sigmoid, reLu, tanh). \n",
    "\n",
    "Making it its own layer makes calculations much faster and easier.\n",
    "\n",
    "$$Y = f(X)$$ \n",
    "\n",
    "$$\\frac{\\delta E}{\\delta X} = \\frac{\\delta E}{\\delta Y} \\cdot f'(X)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation=activation # this is a function.\n",
    "        self.activation_prime=activation_prime # also a function.\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input) # applies inputs to activation function and returns outputs.\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Specific Activation Function:\n",
    "**TanH**: Give description of what tanh good at and used for.\n",
    "$$f(x) = tanh(x)$$\n",
    "\n",
    "$$s'(x) = 1 - [tanh(x)]^2$$\n",
    "**Sigmoid**: Give description of what sigmoid good at and used for.\n",
    "$$s(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$s'(x) = s(x) \\cdot (1-s(x))$$\n",
    "**ReLu**: Give description of what relu good at and used for.\n",
    "$$r(x) = argmax(0,x)$$\n",
    "\n",
    "$$r'(x) = 1 \\text{ ; if } x >= 0$$\n",
    "$$r'(x) = 0 \\text{ ; else}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        sigmoid_prime = lambda x: sigmoid(x) * (1-sigmoid(x))\n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "        \n",
    "class ReLu(Activation):\n",
    "    def __init__(self):\n",
    "        relu = np.max(0,x)\n",
    "        relu_prime = lambda x: 1 if x >= 0 else 0\n",
    "        super().__init__(relu, relu_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error:\n",
    "\n",
    "$y_i$ : an expected output of the model.\n",
    "\n",
    "$\\hat{y}_i$ : an predicted output of the model.\n",
    "\n",
    "$$E = \\frac{1}{n}\\sum_{i}^{n} (y_i - \\hat{y_i})^2$$\n",
    "$$E' = \\frac{2}{n}(\\hat{Y} - Y)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred-y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying It Out:\n",
    "\n",
    "To test this out I followed the video and tried an implementation of a XOR model.\n",
    "XOR is a deceptively simple model, that only has 2 inputs and follows this ruleset:\n",
    "\n",
    "- if $x1 = 0$ and $x2 = 0$ then: $y = 0$\n",
    "\n",
    "- if $x1 = 1$ and $x2 = 0$ then: $y = 1$\n",
    "\n",
    "- if $x1 = 0$ and $x2 = 1$ then: $y = 1$\n",
    "\n",
    "- if $x1 = 1$ and $x2 = 1$ then: $y = 0$\n",
    "\n",
    "The trick is there is no linear way to model this relationship. So it is a good test as our neural network will have to find a nonlinear way to map this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0]\n",
      " [0]]\n",
      "y: [[0]]\n",
      "x: [[0]\n",
      " [1]]\n",
      "y: [[1]]\n",
      "x: [[1]\n",
      " [0]]\n",
      "y: [[1]]\n",
      "x: [[1]\n",
      " [1]]\n",
      "y: [[0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "for x,y in zip(X,Y):\n",
    "    print('x:',x)\n",
    "    print('y:',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.620273\n",
      "2/100, error=0.372152\n",
      "3/100, error=0.279402\n",
      "4/100, error=0.225152\n",
      "5/100, error=0.194102\n",
      "6/100, error=0.174230\n",
      "7/100, error=0.156855\n",
      "8/100, error=0.139415\n",
      "9/100, error=0.122286\n",
      "10/100, error=0.107066\n",
      "11/100, error=0.094817\n",
      "12/100, error=0.085120\n",
      "13/100, error=0.076887\n",
      "14/100, error=0.069449\n",
      "15/100, error=0.062633\n",
      "16/100, error=0.056464\n",
      "17/100, error=0.050976\n",
      "18/100, error=0.046164\n",
      "19/100, error=0.041981\n",
      "20/100, error=0.038358\n",
      "21/100, error=0.035221\n",
      "22/100, error=0.032494\n",
      "23/100, error=0.030113\n",
      "24/100, error=0.028022\n",
      "25/100, error=0.026176\n",
      "26/100, error=0.024535\n",
      "27/100, error=0.023070\n",
      "28/100, error=0.021754\n",
      "29/100, error=0.020567\n",
      "30/100, error=0.019492\n",
      "31/100, error=0.018514\n",
      "32/100, error=0.017621\n",
      "33/100, error=0.016802\n",
      "34/100, error=0.016050\n",
      "35/100, error=0.015357\n",
      "36/100, error=0.014716\n",
      "37/100, error=0.014122\n",
      "38/100, error=0.013570\n",
      "39/100, error=0.013056\n",
      "40/100, error=0.012577\n",
      "41/100, error=0.012129\n",
      "42/100, error=0.011708\n",
      "43/100, error=0.011314\n",
      "44/100, error=0.010944\n",
      "45/100, error=0.010594\n",
      "46/100, error=0.010265\n",
      "47/100, error=0.009954\n",
      "48/100, error=0.009660\n",
      "49/100, error=0.009382\n",
      "50/100, error=0.009117\n",
      "51/100, error=0.008866\n",
      "52/100, error=0.008628\n",
      "53/100, error=0.008401\n",
      "54/100, error=0.008185\n",
      "55/100, error=0.007979\n",
      "56/100, error=0.007782\n",
      "57/100, error=0.007594\n",
      "58/100, error=0.007414\n",
      "59/100, error=0.007242\n",
      "60/100, error=0.007077\n",
      "61/100, error=0.006919\n",
      "62/100, error=0.006767\n",
      "63/100, error=0.006622\n",
      "64/100, error=0.006482\n",
      "65/100, error=0.006347\n",
      "66/100, error=0.006218\n",
      "67/100, error=0.006093\n",
      "68/100, error=0.005973\n",
      "69/100, error=0.005857\n",
      "70/100, error=0.005745\n",
      "71/100, error=0.005638\n",
      "72/100, error=0.005534\n",
      "73/100, error=0.005433\n",
      "74/100, error=0.005336\n",
      "75/100, error=0.005242\n",
      "76/100, error=0.005151\n",
      "77/100, error=0.005062\n",
      "78/100, error=0.004977\n",
      "79/100, error=0.004894\n",
      "80/100, error=0.004814\n",
      "81/100, error=0.004736\n",
      "82/100, error=0.004661\n",
      "83/100, error=0.004588\n",
      "84/100, error=0.004517\n",
      "85/100, error=0.004447\n",
      "86/100, error=0.004380\n",
      "87/100, error=0.004315\n",
      "88/100, error=0.004251\n",
      "89/100, error=0.004190\n",
      "90/100, error=0.004130\n",
      "91/100, error=0.004071\n",
      "92/100, error=0.004014\n",
      "93/100, error=0.003958\n",
      "94/100, error=0.003904\n",
      "95/100, error=0.003852\n",
      "96/100, error=0.003800\n",
      "97/100, error=0.003750\n",
      "98/100, error=0.003701\n",
      "99/100, error=0.003653\n",
      "100/100, error=0.003607\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "network = [\n",
    "    Dense(2, 3),\n",
    "    Tanh(),\n",
    "    Dense(3, 1),\n",
    "    Tanh()\n",
    "]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# train\n",
    "\n",
    "for e in range(epochs):\n",
    "    error = 0\n",
    "    for x,y in zip(X,Y):\n",
    "        # forward\n",
    "        output = x \n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        error += mse(y, output)\n",
    "\n",
    "        # backward\n",
    "        grad = mse_prime(y, output)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "    error /= len(X)\n",
    "    print('%d/%d, error=%f' % (e+1, epochs, error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
