{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn_scripts.layer import Layer\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks:\n",
    "\n",
    "Created by following this video: https://www.youtube.com/watch?v=Lakz2MoHy6o \n",
    "\n",
    "To first begin a look at a convolutional neural network we first need to start with a look at convolutions themselves.\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "The most intuitive way to think about a convolution is to consider a input object say a 3 x 3 matrix with some random values, we'll call it $I$. Then imagine another smaller 2 x 2 matrix that has values only 0.25, we'll call it $K$. If you slid $K$ over the entirety of $I$ and multiplied the correlating values of each matrix and added them to get a new value, you would essentially get the average of each 2 x2 square in $I$. This is essentially the idea of a convolution.\n",
    "\n",
    "The only other things you need to note right now are: \n",
    "\n",
    "- This process is technically a correlation not a convolution to make this a convolution you must rotate $K$ 180 degrees(this is not the same thing as transpose.)\n",
    "- Also, $K$ stands for kernel, which is generally what these matrices will be called.\n",
    "\n",
    "$$conv(I,K) = I \\star rotate180(K)$$\n",
    "\n",
    "<p style=\"text-align: center;\">or</p>\n",
    "\n",
    "$$conv(I,K) = I * K$$\n",
    "\n",
    "### Convolutions in a NN:\n",
    "\n",
    "Inputs, denoted $X$, will be 3-dimensional objects that are almost like several pages of a book. Each page,$X_n$, is a matrix (probably a square matrix) and the amount of pages can be thought of as the depth, denoted $n$. The elements of the inputs represent activations for a given input. Each element denoted by: $$x_{i,j}^n \\in X_n$$\n",
    "\n",
    "The kernels object, denoted $K$, will be 4-dimensional, with a given kernel,$K_{dn}$, having $n$ 2d matrices to apply to each the matrix at each depth in the input, and then there can be multiple kernels in a kernel which each are denoted by $d$. The elements of kernels represent weights that are attached to the inputs to calculate a weighted sum. Weights denoted by: $$w_{i,j}^d \\in K_{dn}$$\n",
    "\n",
    "Then for each kernel the layer contains, a separate bias matrix, denoted $B_d$ will be added to the resulting weighted sums for each output. Elements denoted: $$b_{i,j}^d \\in B_d$$\n",
    "\n",
    "Finally to show how the full output matrix of the convolution layer, $Y_d$ is mathematically represented: \n",
    "$$Y_d = B_d + X_1 \\star K_{1d} + ... + X_n \\star K_{nd}$$\n",
    "\n",
    "$$Y = B + X \\cdot|\\star K $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernel_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernel_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\") \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: propagate backwards\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propogation:\n",
    "\n",
    "With a given output and error function, we can easily calculate the derivate $\\frac{\\delta E}{\\delta K_{ij}}$ with \n",
    "\n",
    "$$\\frac{\\delta E}{\\delta K_{ij}} = X_j \\star \\frac{\\delta E}{\\delta Y_i}$$\n",
    "\n",
    "Using some similar calculations we get: \n",
    "$$\\frac{\\delta E}{\\delta B_{i}} = \\frac{\\delta E}{\\delta Y_i}$$\n",
    "\n",
    "$$\\frac{\\delta E}{\\delta X_{j}} = \\sum_{i=1}^n \\frac{\\delta E}{\\delta Y_i} \\star_{full} K_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernel_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernel_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\") \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernel_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                kernels_gradient[i,j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                input_gradient[j] += signal.correlate2d(output_gradient[i], self.kernels[i,j], \"full\")\n",
    "        \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Layer:\n",
    "\n",
    "At the end of a CNN, it is very common to have one or a few dense layers to make final predictions. To enter into the dense layer the data must be shaped in a 1-dimensional column so to turn our 3-d output objects into this shape we use a reshape layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Implementation:\n",
    "\n",
    "Bringing it all together, first we'll get the MNIST Dataset from the keras.datasets packages. The MNIST Dataset is the definitive neural network dataset, comrprised of a million labeled 28 x 28 images of handwritten digits 0-9. The general problem is to take the pixel activation values as a neuron each and then try to get a model that can accurately identify the handwritten digits. \n",
    "\n",
    "For my implementation I chose to limit it to only 1000 training examples and 20 test examples, to avoid extreme training times. To prepare the data for the implementation it involves first normalizing the pixel values to 0-1. Then adjusting the y values to be one-hot encoded to make the model a classification model rather than a regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cobyw\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! pip install keras\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    # reshape and normalize input data\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    # encode output which is a number in range [0,9] into a vector of size 10\n",
    "    # e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    y = np.eye(10)[y]\n",
    "    y = y.reshape(y.shape[0], 10, 1)\n",
    "    return x[:limit], y[:limit]\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then all I need to do is add my layers to a network object. For this I chose to have 2 convolutional layers, with 6 kernels and with a size of 3 x 3 each. Finally put the convolutional features through 2 Dense layers to finally choose its answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15, error=0.7598488239180784\n",
      "2/15, error=0.4424885477765716\n",
      "3/15, error=0.35647294572113924\n",
      "4/15, error=0.32698257429738714\n",
      "5/15, error=0.3093853603016622\n",
      "6/15, error=0.3014377847669694\n",
      "7/15, error=0.29522329144716214\n",
      "8/15, error=0.28959494013372933\n",
      "9/15, error=0.2861134354123164\n",
      "10/15, error=0.28169636721567515\n",
      "11/15, error=0.2766333799671089\n",
      "12/15, error=0.27195295217174664\n",
      "13/15, error=0.2674354607087865\n",
      "14/15, error=0.2641227867620214\n",
      "15/15, error=0.2607030438181997\n"
     ]
    }
   ],
   "source": [
    "from nn_scripts.network import *\n",
    "from nn_scripts.dense import Dense\n",
    "from nn_scripts.activations import Sigmoid, Tanh\n",
    "from nn_scripts.loss import *\n",
    "\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 6),\n",
    "    Sigmoid(),\n",
    "    Reshape((6, 26, 26), (6 * 26 * 26, 1)),\n",
    "    Dense(6 * 26 * 26, 100),\n",
    "    Tanh(),\n",
    "    Dense(100, 10),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "train(network, binary_cross_entropy, binary_cross_entropy_prime, x_train, y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training mse is supposedly around 0.25, which is quite decent, to make sure the model was overfitting, I put the test samples through and achieved an error of about 0.35 which I am quite happy with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = [predict(network,input) for input in x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.16214652349750322\n",
      "Error: 0.3435895208736797\n",
      "Error: 0.5039025401996056\n",
      "Error: 0.6014211317786741\n",
      "Error: 0.1522104957956137\n",
      "Error: 0.39957298504754213\n",
      "Error: 0.15222141042462417\n",
      "Error: 0.3625097403488424\n",
      "Error: 0.35964982726072775\n",
      "Error: 0.26888848285858535\n",
      "Error: 0.48775162311442866\n",
      "Error: 0.7277393383319422\n",
      "Error: 0.25594468139299553\n",
      "Error: 0.16832549869195979\n",
      "Error: 0.19912678954531687\n",
      "Error: 0.7791157060627018\n",
      "Error: 0.1542535710251805\n",
      "Error: 0.36724582101680486\n",
      "Error: 0.45873283939721965\n",
      "Error: 0.15224237067692975\n",
      "Total Average Error: 0.3528295448670439\n"
     ]
    }
   ],
   "source": [
    "error = 0.0\n",
    "for y,y_hat in zip(y_test,pred_test):\n",
    "    answer_error = binary_cross_entropy(y, y_hat)\n",
    "    print('Error:',answer_error)\n",
    "    error += answer_error\n",
    "print('Total Average Error:',error / len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
